{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "from sltk import sltk\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpus\n",
    "with open(\"data/sin.csv\", \"r\") as f:\n",
    "    corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_chars(txt):\n",
    "    result = 0\n",
    "    for char in txt:\n",
    "        if(char != \"\" or char != \" \"):\n",
    "            result = result + 1\n",
    "    return result\n",
    "\n",
    "num_char = count_chars(corpus)\n",
    "num_words = len(corpus.split(' '))\n",
    "\n",
    "print(f'{num_char} {num_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK tokenization\n",
    "def nltk_tokenizer():\n",
    "    start_time = time.time()\n",
    "    nltk_tokens = nltk.word_tokenize(corpus)\n",
    "    nltk_time = time.time() - start_time\n",
    "    nltk_compression = len(nltk_tokens) / (num_char + num_words)\n",
    "    print(\"NLTK: Time = {:.2f}s, Compression = {:.2f}\".format(nltk_time, nltk_compression))\n",
    "    return nltk_time, nltk_compression, len(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_time, nltk_compression, nltk_tokens = nltk_tokenizer()\n",
    "print(nltk_time, nltk_compression, nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy tokenization\n",
    "def spacy_tokenizer():\n",
    "    spacy_nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    start_time = time.time()\n",
    "    spacy_nlp.max_length = 1_500_000\n",
    "    spacy_doc = spacy_nlp(corpus)\n",
    "    spacy_tokens = [token.text for token in spacy_doc]\n",
    "    spacy_time = time.time() - start_time\n",
    "    spacy_compression = len(spacy_tokens) / (num_char + num_words)\n",
    "    print(\"spaCy: Time = {:.2f}s, Compression = {:.2f}\".format(spacy_time, spacy_compression))\n",
    "    return spacy_time, spacy_compression, len(spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_time, spacy_compression, spacy_tokens = spacy_tokenizer()\n",
    "print(spacy_time, spacy_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization\n",
    "def bertTokenizer():\n",
    "    start_time = time.time()\n",
    "    # initialize\n",
    "    tokenizer = BertWordPieceTokenizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=False,\n",
    "        strip_accents=False,\n",
    "        lowercase=False\n",
    "    )\n",
    "    # and train\n",
    "    tokenizer.train(files='data/sin.csv', vocab_size=30_000, min_frequency=2,\n",
    "                    limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                    special_tokens=[\n",
    "                        '[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "\n",
    "    # os.mkdir('./bert')\n",
    "\n",
    "    tokenizer.save_model('./bert', 'bert')\n",
    "\n",
    "    # tokenize\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"./bert\")\n",
    "    bert_tokens = tokenizer.tokenize(corpus)\n",
    "    bert_time = time.time() - start_time\n",
    "    bert_compression = len(bert_tokens) / (num_char + num_words)\n",
    "    print(\"BERT: Time = {:.2f}s, Compression = {:.2f}\".format(bert_time, bert_compression))\n",
    "    # print(bert_tokens)\n",
    "    return bert_time, bert_compression, len(bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_time, bert_compression, bert_tokens = bertTokenizer()\n",
    "print(bert_time, bert_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sltk tokenizer\n",
    "def sltTokenizer():\n",
    "    start_time = time.time()\n",
    "\n",
    "    sentences = sltk.splitSentences(corpus)\n",
    "    sltk_tokens = sltk.buildVocab(sentences)\n",
    "    f = open(\"models/vocab.txt\", \"w\")\n",
    "    f.write('\\n'.join(sltk_tokens))\n",
    "    f.close()\n",
    "\n",
    "    #tokenize\n",
    "    # sltk_tokens = sltk.tokenize('models/vocab.txt', sentences)\n",
    "    sltk_time = time.time() - start_time\n",
    "    sltk_compression = len(sltk_tokens) / (num_char + num_words)\n",
    "    print(\"SLTK: Time = {:.2f}s, Compression = {:.2f}\".format(sltk_time, sltk_compression))\n",
    "    return sltk_time, sltk_compression, len(sltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sltk_time, sltk_compression, sltk_tokens = sltTokenizer()\n",
    "print(sltk_time, sltk_compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "sns.set_theme()\n",
    "speed = [nltk_time, spacy_time, bert_time, sltk_time]\n",
    "compression = [nltk_compression, spacy_compression, bert_compression, sltk_compression]\n",
    "tokenizers = [\"NLTK\", \"spaCy\", \"BERT\", \"SLTK\"]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "\n",
    "color = \"tab:orange\"\n",
    "ax1.set_xlabel(\"Tokenizer\")\n",
    "ax1.set_ylabel(\"Speed (s)\", color=color)\n",
    "ax1.bar(tokenizers, speed, color=color)\n",
    "ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = \"tab:blue\"\n",
    "ax2.set_ylabel(\"Compression Factor\", color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(tokenizers, compression, color=color, marker='o', label='Compression Factor')\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = (\"NLTK\", \"spaCy\", \"BERT\", \"SLTK\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [nltk_tokens, spacy_tokens, bert_tokens, sltk_tokens]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Vacabulary Size')\n",
    "plt.title('Number of Tokens Generated')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
